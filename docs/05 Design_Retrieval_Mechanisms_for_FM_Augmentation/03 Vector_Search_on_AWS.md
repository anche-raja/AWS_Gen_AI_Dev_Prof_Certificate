## Vector Search on AWS ğŸ”

Vector search lets you find results by **meaning**, not just matching words. This note connects foundation model embeddings, similarity metrics, and the main AWS services youâ€™ll see on the exam.

---

## 1. From text to embeddings ğŸ§ â¡ï¸ğŸ”¢

Foundation models turn text into highâ€‘dimensional vectors (embeddings) in four logical steps:
- **Preprocessing & tokenization**
  - Normalize text (case, punctuation, special chars) and split into tokens (subwords/word pieces).
- **Contextual encoding**
  - Multiple transformer layers build context across tokens (syntax + semantics).
- **Vector generation**
  - Final hidden states are combined into dense vectors (hundredsâ€“thousands of dimensions).
- **Optimization & output**
  - Optional dimensionality reduction / normalization to make embeddings practical for search.

Result: Similar meanings â†’ **nearby vectors** in a highâ€‘dimensional space, even if the exact words differ.

---

## 2. Semantic similarity & vector space geometry ğŸ“

- **Key ideas**
  - Embeddings live in a **high-dimensional vector space**.
  - **Semantic similarity â‰ˆ geometric proximity** (close vectors = related meaning).
  - Related concepts form **clusters**; unrelated ones are far apart.
- **Common similarity metrics**
  - **Cosine similarity** â€“ angle between vectors (âˆ’1 to 1). Most common for semantic search.
  - **Euclidean distance** â€“ straight-line distance; considers magnitude.
  - **Dot product** â€“ combines angle + magnitude; used in some ANN libraries.
  - **Manhattan distance** â€“ sum of absolute differences (less common here).
- **Dimensionality tradeoffs**
  - Higher dimensions â†’ richer nuance, more storage/compute.
  - Lower dimensions â†’ faster, cheaper, but less expressive.
  - Typical ranges: **128â€“1536**; pick based on accuracy vs cost/latency needs.

---

## 3. Vector search vs keyword search âš–ï¸

**Vector search**
- Matches on **semantic similarity**, not exact terms.
- Handles synonyms, paraphrases, and even crossâ€‘language (with multilingual models).
- Great for:
  - Natural language queries (â€œHow can I reduce S3 costs?â€).
  - FAQ matching and support assistants.
  - Recommendations based on content similarity.
  - Research / discovery where users donâ€™t know exact terms.

**Keyword search**
- Matches **exact words/phrases**; often supports Boolean logic.
- Very fast and easy to index.
- Great for:
  - IDs / codes (error IDs, ticket numbers, SKUs).
  - Compliance / eâ€‘discovery where **exact phrases** matter.
  - Highly structured fields and filters.

**Hybrid search**
- Combine **vector + keyword**:
  - Use embeddings for semantic matches.
  - Use keywords for exact matches and filters.
  - Fuse/weight results from both for robust behavior.

Exam anchor: For **natural language + ambiguity**, favor vector or hybrid; for **exact identifiers & compliance**, favor keyword (possibly with hybrid).

---

## 4. AWS vector search options ğŸ§±

High level choices:

- **Amazon OpenSearch Service (with kâ€‘NN/ANN)**
  - Best for: **highâ€‘performance, lowâ€‘latency semantic search** with rich analytics and filters.
  - Capabilities:
    - Native kâ€‘NN (multiple ANN algorithms).
    - Realâ€‘time indexing + updates.
    - Advanced filtering, aggregations, dashboards.
    - Horizontal scaling via sharding + replicas.
  - Example pattern:
    - Store embeddings in a vector field.
    - Run a kâ€‘NN query with your query embedding to get **topâ€‘k similar docs**.

- **Amazon Aurora PostgreSQL with pgvector**
  - Best for: Apps that already use **PostgreSQL** and need **relational + vector** in one place.
  - Capabilities:
    - Vector type and indexes inside Postgres.
    - JOIN vectors with relational tables in SQL.
    - Managed HA, backups, read replicas from Aurora.
  - Good for:
    - Transactional systems needing light/medium vector search without extra infra.

- **Amazon S3 Vectors**
  - Best for: Largeâ€‘scale, **costâ€‘efficient vector storage** integrated with S3.
  - Capabilities:
    - Store embeddings alongside data in S3.
    - Serverless, payâ€‘perâ€‘use semantics.
  - Typically used when you:
    - Need to keep cost low at very large scale.
    - Are comfortable orchestrating your own retrieval logic.

- **Amazon Bedrock Knowledge Bases**
  - Best for: **Managed RAG** â€“ fast path to questionâ€‘answering over your data.
  - Capabilities:
    - Fully managed ingestion â†’ chunking â†’ embedding â†’ vector storage.
    - Configurable sources (S3, Confluence, etc.) and embedding models.
    - Builtâ€‘in retriever API wired to Bedrock FMs.
  - Tradeoffs:
    - Minimal ops, but less lowâ€‘level control than custom OpenSearch/pgvector setups.

---

## 5. Choosing the right service ğŸ§­

Use these guiding questions:
- **Do you need rich filters, dashboards, and subâ€‘ms latency?**
  - â†’ **OpenSearch Service**.
- **Already on PostgreSQL and want relational + vector in one place?**
  - â†’ **Aurora PostgreSQL with pgvector**.
- **Need ultraâ€‘lowâ€‘cost, largeâ€‘scale vector storage and can build your own retrieval logic?**
  - â†’ **S3 Vectors**.
- **Want a managed RAG solution with minimal plumbing?**
  - â†’ **Bedrock Knowledge Bases**.

Always align:
- **Performance** (latency, throughput).
- **Scale** (data volume, growth).
- **Cost** (storage + queries + ops).
- **Operational complexity** (who will run and tune it).

If you can explain **how embeddings work**, **why cosine similarity is used**, and **which AWS vector service fits which scenario**, youâ€™re in great shape for this part of the exam.


